<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Duy P. Nguyen </title> <meta name="author" content="Duy P. Nguyen"> <meta name="description" content=""> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://buzinguyen.github.io/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/learning/">learning </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Duy</span> P. Nguyen </h1> <p class="desc"><a href="https://ece.princeton.edu/people/duy-phuong-nguyen" rel="external nofollow noopener" target="_blank">ECE, Princeton University</a>. duyn@princeton.edu</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/personal_profile-480.webp 480w,/assets/img/personal_profile-800.webp 800w,/assets/img/personal_profile-1400.webp 1400w," type="image/webp" sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw"> <img src="/assets/img/personal_profile.jpg?6c50c4b1ebb5590a4316f11f43c098e5" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="personal_profile.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <p>Princeton University</p> <p>Electrical and Computer Eng</p> <p>Engineering Quad, Room B204</p> </div> </div> <div class="clearfix"> <p>I am a PhD Candidate in Electrical and Computer Engineering at <a href="https://www.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton University</a>, advised by <a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" rel="external nofollow noopener" target="_blank">Prof. Jaime Fernández Fisac</a> in the <a href="https://saferobotics.princeton.edu/" rel="external nofollow noopener" target="_blank">Safe Robotics Lab</a>.</p> <p>My research focuses on <strong>scaling safe reinforcement learning for high-dimensional robotic systems</strong>, enabling robots to operate robustly in the real world and collaborate effectively with humans. To this end, I develop learning and control frameworks that leverage <strong>world models, adversarial imagination, and closed-loop foundation-model fine-tuning</strong>, aiming to bridge the gap between theoretical safety guarantees and real-world deployment.</p> <p>I ground my work in both academic and industrial settings, ranging from <strong>closed-loop RL fine-tuning pipelines for autonomous driving</strong> developed during my internship at <a href="https://waymo.com/" rel="external nofollow noopener" target="_blank">Waymo</a>, to <strong>real-time introspective safety mechanisms</strong> field-tested in the <a href="https://www.darpa.mil/research/programs/learning-introspective-control" rel="external nofollow noopener" target="_blank">DARPA LINC program</a>.</p> <p><b><u>I am currently on the job market</u></b> and actively seeking full-time opportunities in robotics and autonomy, as well as research collaborations.</p> </div> <h2> <a href="/news/" style="color: inherit">news</a> </h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Dec 10, 2025</th> <td> Our papers “<a href="https://arxiv.org/abs/2510.18082" rel="external nofollow noopener" target="_blank">Provably Optimal Reinforcement Learning under Safety Filtering</a>” and “<a href="https://arxiv.org/abs/2510.13727" rel="external nofollow noopener" target="_blank">From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails</a>” were accepted to <a href="https://www.iaseai.org/our-programs/iaseai26" rel="external nofollow noopener" target="_blank">The International Association for Safe &amp; Ethical AI (IASEAI’26)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 25, 2025</th> <td> I’m featured in the <a href="https://csml.princeton.edu/" rel="external nofollow noopener" target="_blank">Princeton Center for Statistics and Machine Learning</a> news for my work on safe robotics — check out <a href="https://csml.princeton.edu/news/duy-nguyen-building-robots-won%E2%80%99t-let-you-down" rel="external nofollow noopener" target="_blank">the article on the CSML website</a>! </td> </tr> <tr> <th scope="row" style="width: 20%">May 27, 2025</th> <td> I will be joining <a href="https://waymo.com/" rel="external nofollow noopener" target="_blank">Waymo</a> as a research intern in Summer 2025. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 20, 2024</th> <td> Our paper “<a href="https://arxiv.org/abs/2405.00846" rel="external nofollow noopener" target="_blank">Gameplay Filters: Robust Zero-Shot Safety though Adversarial Imagination</a>” was accepted to <a href="https://2024.corl.org/" rel="external nofollow noopener" target="_blank">Annual Conference on Robot Learning (CoRL 2024)</a> for oral presentation. </td> </tr> <tr> <th scope="row" style="width: 20%">May 31, 2023</th> <td> I passed my General Exam! I am thankful for the support from <a href="https://ece.princeton.edu/people/jaime-fernandez-fisac" rel="external nofollow noopener" target="_blank">Prof. Jaime Fernández Fisac</a> and the <a href="https://saferobotics.princeton.edu/" rel="external nofollow noopener" target="_blank">Safe Robotics Lab</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Mar 15, 2023</th> <td> Our paper <a href="https://saferobotics.princeton.edu/research/isaacs" rel="external nofollow noopener" target="_blank">“ISAACS: Iterative Soft Adversarial Actor-Critic for Safety”</a> was accepted to <a href="https://l4dc.seas.upenn.edu/" rel="external nofollow noopener" target="_blank">Learning for Dynamics and Control (L4DC 2023)</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Nov 04, 2022</th> <td> Our paper <a href="https://saferoboticslab.github.io/SimLabReal/" rel="external nofollow noopener" target="_blank">“Sim-to-Lab-to-Real: Safe Reinforcement Learning with Shielding and Generalization Guarantees”</a> was accepted to <a href="https://www.sciencedirect.com/special-issue/10C887PF03L" rel="external nofollow noopener" target="_blank">Special Issue on Risk-aware Autonomous Systems: Theory and Practice, Artificial Intelligence</a>. </td> </tr> <tr> <th scope="row" style="width: 20%">Sep 01, 2022</th> <td> Received a teaching assistant award from Princeton University for developing the new <a href="https://ece.princeton.edu/news/robot-trucks-drive-students-solve-real-problems-modern-robotics" rel="external nofollow noopener" target="_blank">Intelligent Robotic Systems</a> course. Thank you Professor Fisac, Zixu and Kai-Chieh! </td> </tr> </table> </div> </div> <h2> <a href="/blog/" style="color: inherit">latest posts</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">Nov 08, 2024</th> <td> <a class="news-title" href="/blog/2024/corl-2024-demo-experience/">CoRL 2024 Demo Recap</a> </td> </tr> </table> </div> </div> <h2> <a href="/projects/" style="color: inherit">selected projects</a> </h2> <div class="projects"> <div class="row row-cols-1 row-cols-md-3"> <div class="col"> <a href="/projects/DARPA-LINC-phase0/"> <div class="card h-100 hoverable"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/wedge-terrain-preview-480.webp 480w,/assets/img/wedge-terrain-preview-800.webp 800w,/assets/img/wedge-terrain-preview-1400.webp 1400w," type="image/webp" sizes="250px"> <img src="/assets/img/wedge-terrain-preview.gif" class="card-img-top" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="card-body"> <h2 class="card-title">DARPA LINC Phase 0</h2> <p class="card-text">Learning Introspective Control for safety-critical field robots</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="col"> <a href="/projects/gameplay-filters/"> <div class="card h-100 hoverable"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/robert-kicking-short-zoom-480.webp 480w,/assets/img/robert-kicking-short-zoom-800.webp 800w,/assets/img/robert-kicking-short-zoom-1400.webp 1400w," type="image/webp" sizes="250px"> <img src="/assets/img/robert-kicking-short-zoom.gif" class="card-img-top" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="card-body"> <h2 class="card-title">ISAACS and Gameplay Filters</h2> <p class="card-text">Safety Filter synthesis and deployment for high-order dynamical systems</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> <div class="col"> <a href="/projects/DARPA-LINC-phase1/"> <div class="card h-100 hoverable"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/linc-crane-helicopter-short-480.webp 480w,/assets/img/linc-crane-helicopter-short-800.webp 800w,/assets/img/linc-crane-helicopter-short-1400.webp 1400w," type="image/webp" sizes="250px"> <img src="/assets/img/linc-crane-helicopter-short.gif" class="card-img-top" width="100%" height="auto" alt="project thumbnail" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="card-body"> <h2 class="card-title">DARPA LINC Phase 1</h2> <p class="card-text">Learning Introspective Control for Replenishment at Sea missions.</p> <div class="row ml-1 mr-1 p-0"> </div> </div> </div> </a> </div> </div> </div> <h2> <a href="/publications/" style="color: inherit">selected publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://www.corl.org/" rel="external nofollow noopener" target="_blank">CoRL</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/gameplay-filters.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gameplay-filters.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2024gameplay" class="col-sm-8"> <div class="title">Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination</div> <div class="author"> <em>Duy P. Nguyen<sup>*</sup></em>, Kai-Chieh Hsu<sup>*</sup>, Wenhao Yu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jie Tan, Jaime Fernández Fisac' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 8th Annual Conference on Robot Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://saferobotics.princeton.edu/research/gameplay-filter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2405.00846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-18-4285F4?logo=googlescholar&amp;labelColor=beige" alt="18 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Despite the impressive recent advances in learning-based robot control, ensuring robustness to out-of-distribution conditions remains an open challenge. Safety filters can, in principle, keep arbitrary control policies from incurring catastrophic failures by overriding unsafe actions, but existing solutions for complex (e.g., legged) robot dynamics do not span the full motion envelope and instead rely on local, reduced-order models. These filters tend to overly restrict agility and can still fail when perturbed away from nominal conditions. This paper presents the gameplay filter, a new class of predictive safety filter that continually plays out hypothetical matches between its simulation-trained safety strategy and a virtual adversary co-trained to invoke worst-case events and sim-to-real error, and precludes actions that would cause failures down the line. We demonstrate the scalability and robustness of the approach with a first-of-its-kind full-order safety filter for (36-D) quadrupedal dynamics. Physical experiments on two different quadruped platforms demonstrate the superior zero-shot effectiveness of the gameplay filter under large perturbations such as tugging and unmodeled terrain. Experiment videos and open-source software are available online: https://saferobotics.org/research/gameplay-filter</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://openreview.net/group?id=L4DC.org" rel="external nofollow noopener" target="_blank">L4DC</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ISAACS.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ISAACS.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pmlr-v211-hsu23a" class="col-sm-8"> <div class="title">ISAACS: Iterative Soft Adversarial Actor-Critic for Safety</div> <div class="author"> Kai-Chieh Hsu<sup>*</sup>, <em>Duy Phuong Nguyen<sup>*</sup></em>, and Jaime Fernàndez Fisac </div> <div class="periodical"> <em>In Proceedings of The 5th Annual Learning for Dynamics and Control Conference</em>, 15–16 jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://saferobotics.princeton.edu/research/isaacs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.mlr.press/v211/hsu23a/hsu23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/RlPQR044pEQ?si=BO3vguUSvZc81r9i" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:Tyk-4Ss8FVUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-43-4285F4?logo=googlescholar&amp;labelColor=beige" alt="43 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable "deep" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial "disturbance" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer’s uncertainty. While the learned control policy does not intrinsically guarantee safety, it is used to construct a real-time safety filter (or shield) with robust safety guarantees based on forward reachability rollouts. This shield can be used in conjunction with a safety-agnostic control policy, precluding any task-driven actions that could result in loss of safety. We evaluate our learning-based safety approach in a 5D race car simulator, compare the learned safety policy to the numerically obtained optimal solution, and empirically validate the robust safety guarantee of our proposed safety shield against worst-case model discrepancy.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://www.iaseai.org/" rel="external nofollow noopener" target="_blank">IASEAI</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/porl.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="porl.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="oh2025provablyoptimalreinforcementlearning" class="col-sm-8"> <div class="title">Provably Optimal Reinforcement Learning under Safety Filtering</div> <div class="author"> Donggeon David Oh<sup>*</sup>, <em>Duy P. Nguyen<sup>*</sup></em>, Haimin Hu, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jaime F. Fisac' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2510.18082" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:KlAtU1dfN6UC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Recent advances in reinforcement learning (RL) enable its use on increasingly complex tasks, but the lack of formal safety guarantees still limits its application in safety-critical settings. A common practical approach is to augment the RL policy with a safety filter that overrides unsafe actions to prevent failures during both training and deployment. However, safety filtering is often perceived as sacrificing performance and hindering the learning process. We show that this perceived safety-performance tradeoff is not inherent and prove, for the first time, that enforcing safety with a sufficiently permissive safety filter does not degrade asymptotic performance. We formalize RL safety with a safety-critical Markov decision process (SC-MDP), which requires categorical, rather than high-probability, avoidance of catastrophic failure states. Additionally, we define an associated filtered MDP in which all actions result in safe effects, thanks to a safety filter that is considered to be a part of the environment. Our main theorem establishes that (i) learning in the filtered MDP is safe categorically, (ii) standard RL convergence carries over to the filtered MDP, and (iii) any policy that is optimal in the filtered MDP-when executed through the same filter-achieves the same asymptotic return as the best safe policy in the SC-MDP, yielding a complete separation between safety enforcement and performance optimization. We validate the theory on Safety Gymnasium with representative tasks and constraints, observing zero violations during training and final performance matching or exceeding unfiltered baselines. Together, these results shed light on a long-standing question in safety-filtered learning and provide a simple, principled recipe for safe RL: train and deploy RL policies with the most permissive safety filter that is available.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://www.sciencedirect.com/journal/artificial-intelligence" rel="external nofollow noopener" target="_blank">AIJ</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/sim-to-lab-to-real.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="sim-to-lab-to-real.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="HSU2023103811" class="col-sm-8"> <div class="title">Sim-to-Lab-to-Real: Safe reinforcement learning with shielding and generalization guarantees</div> <div class="author"> Kai-Chieh Hsu, Allen Z. Ren, <em>Duy P. Nguyen</em>, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Anirudha Majumdar, Jaime F. Fisac' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>Artificial Intelligence</em>, 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/https://doi.org/10.1016/j.artint.2022.103811" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://saferoboticslab.github.io/SimLabReal/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2201.08355" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:IjCSPb-OGe4C" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-74-4285F4?logo=googlescholar&amp;labelColor=beige" alt="74 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Safety is a critical component of autonomous systems and remains a challenge for learning-based policies to be utilized in the real world. In particular, policies learned using reinforcement learning often fail to generalize to novel environments due to unsafe behavior. In this paper, we propose Sim-to-Lab-to-Real to bridge the reality gap with a probabilistically guaranteed safety-aware policy distribution. To improve safety, we apply a dual policy setup where a performance policy is trained using the cumulative task reward and a backup (safety) policy is trained by solving the Safety Bellman Equation based on Hamilton-Jacobi (HJ) reachability analysis. In Sim-to-Lab transfer, we apply a supervisory control scheme to shield unsafe actions during exploration; in Lab-to-Real transfer, we leverage the Probably Approximately Correct (PAC)-Bayes framework to provide lower bounds on the expected performance and safety of policies in unseen environments. Additionally, inheriting from the HJ reachability analysis, the bound accounts for the expectation over the worst-case safety in each environment. We empirically study the proposed framework for ego-vision navigation in two types of indoor environments with varying degrees of photorealism. We also demonstrate strong generalization performance through hardware experiments in real indoor spaces with a quadrupedal robot. See this https URL for supplementary material.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="http://ifrr.org/wafr/" rel="external nofollow noopener" target="_blank">WAFR</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/magics.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="magics.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="wang2024magicsadversarialrlminimax" class="col-sm-8"> <div class="title">MAGICS: Adversarial RL with Minimax Actors Guided by Implicit Critic Stackelberg for Convergent Neural Synthesis of Robot Safety</div> <div class="author"> Justin Wang, Haimin Hu, <em>Duy Phuong Nguyen</em>, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jaime Fernández Fisac' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2409.13867" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:WF5omc3nYNoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-8-4285F4?logo=googlescholar&amp;labelColor=beige" alt="8 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>While robust optimal control theory provides a rigorous framework to compute robot control policies that are provably safe, it struggles to scale to high-dimensional problems, leading to increased use of deep learning for tractable synthesis of robot safety. Unfortunately, existing neural safety synthesis methods often lack convergence guarantees and solution interpretability. In this paper, we present Minimax Actors Guided by Implicit Critic Stackelberg (MAGICS), a novel adversarial reinforcement learning (RL) algorithm that guarantees local convergence to a minimax equilibrium solution. We then build on this approach to provide local convergence guarantees for a general deep RL-based robot safety synthesis algorithm. Through both simulation studies on OpenAI Gym environments and hardware experiments with a 36-dimensional quadruped robot, we show that MAGICS can yield robust control policies outperforming the state-of-the-art neural safety synthesis methods.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://www.iaseai.org/" rel="external nofollow noopener" target="_blank">IASEAI</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/refusal-to-recovery.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="refusal-to-recovery.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pandya2025refusalrecoverycontroltheoreticapproach" class="col-sm-8"> <div class="title">From Refusal to Recovery: A Control-Theoretic Approach to Generative AI Guardrails</div> <div class="author"> Ravi Pandya, Madison Bland, <em>Duy P. Nguyen</em>, and <span class="more-authors" title="click to view 3 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '3 more authors' ? 'Changliu Liu, Jaime Fernández Fisac, Andrea Bajcsy' : '3 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">3 more authors</span> </div> <div class="periodical"> 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://arxiv.org/pdf/2510.13727" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:kNdYIx-mwKoC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-0-4285F4?logo=googlescholar&amp;labelColor=beige" alt="0 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Generative AI systems are increasingly assisting and acting on behalf of end users in practical settings, from digital shopping assistants to next-generation autonomous cars. In this context, safety is no longer about blocking harmful content, but about preempting downstream hazards like financial or physical harm. Yet, most AI guardrails continue to rely on output classification based on labeled datasets and human-specified criteria,making them brittle to new hazardous situations. Even when unsafe conditions are flagged, this detection offers no path to recovery: typically, the AI system simply refuses to act–which is not always a safe choice. In this work, we argue that agentic AI safety is fundamentally a sequential decision problem: harmful outcomes arise from the AI system’s continually evolving interactions and their downstream consequences on the world. We formalize this through the lens of safety-critical control theory, but within the AI model’s latent representation of the world. This enables us to build predictive guardrails that (i) monitor an AI system’s outputs (actions) in real time and (ii) proactively correct risky outputs to safe ones, all in a model-agnostic manner so the same guardrail can be wrapped around any AI model. We also offer a practical training recipe for computing such guardrails at scale via safety-critical reinforcement learning. Our experiments in simulated driving and e-commerce settings demonstrate that control-theoretic guardrails can reliably steer LLM agents clear of catastrophic outcomes (from collisions to bankruptcy) while preserving task performance, offering a principled dynamic alternative to today’s flag-and-block guardrails.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://ras.papercept.net/conferences/scripts/start.pl" rel="external nofollow noopener" target="_blank">ICRA</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/reach-avoid-games-demo.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="reach-avoid-games-demo.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="reach-avoid-games" class="col-sm-8"> <div class="title">Back to the Future: Efficient, Time-Consistent Solutions in Reach-Avoid Games</div> <div class="author"> Dennis R. Anthony, <em>Duy P. Nguyen</em>, David Fridovich-Keil, and <span class="more-authors" title="click to view 1 more author" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '1 more author' ? 'Jaime F. Fisac' : '1 more author'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">1 more author</span> </div> <div class="periodical"> <em>In 2022 International Conference on Robotics and Automation (ICRA)</em>, 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://doi.org/10.1109/ICRA46639.2022.9812243" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">DOI</a> <a href="https://arxiv.org/pdf/2109.07673" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:UeHWp8X0CEIC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-4-4285F4?logo=googlescholar&amp;labelColor=beige" alt="4 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>We study the class of reach-avoid dynamic games in which multiple agents interact noncooperatively, and each wishes to satisfy a distinct target criterion while avoiding a failure criterion. Reach-avoid games are commonly used to express safety-critical optimal control problems found in mobile robot motion planning. Here, we focus on finding time-consistent solutions, in which future motion plans remain optimal even when a robot diverges from the plan early on due to, e.g., intrinsic dynamic uncertainty or extrinsic environment disturbances. Our main contribution is a computationally-efficient algorithm for multi-agent reach-avoid games which renders time-consistent solutions for all players. We demonstrate our approach in two- and three-player simulated driving scenarios, in which our method provides safe control strategies for all agents.</p> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="/assets/pdf/cv_01072026.pdf" title="CV" target="_blank"><i class="ai ai-cv"></i></a> <a href="mailto:%64%75%79%6E@%70%72%69%6E%63%65%74%6F%6E.%65%64%75" title="email"><i class="fa-solid fa-envelope"></i></a> <a href="https://github.com/buzi-princeton" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-github"></i></a> <a href="https://www.linkedin.com/in/buzinguyen" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-linkedin"></i></a> <a href="https://scholar.google.com/citations?user=BpR72EcAAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> </div> <div class="contact-note">Feel free to reach out to me via email for any inquiries or collaborations. </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Duy P. Nguyen. Last updated: January 17, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>