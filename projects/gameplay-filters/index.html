<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> ISAACS and Gameplay Filters | Duy P. Nguyen </title> <meta name="author" content="Duy P. Nguyen"> <meta name="description" content="Safety Filter synthesis and deployment for high-order dynamical systems"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A4%96&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://buzinguyen.github.io/projects/gameplay-filters/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Duy</span> P. Nguyen </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/projects/">projects <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/learning/">learning </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">ISAACS and Gameplay Filters</h1> <p class="post-description">Safety Filter synthesis and deployment for high-order dynamical systems</p> </header> <article> <h1 id="isaacs">ISAACS</h1> <p>ISAACS (Iterative Soft Adversarial Actor-Critic for Safety) <a class="citation" href="#pmlr-v211-hsu23a">(Hsu* et al., 2023)</a> is a new game-theoretic reinforcement learning scheme for approximate safety analysis, whose simulation-trained control policies can be efficiently converted at runtime into robust safety-certified control strategies, allowing robots to plan and operate with safety guarantees in the physical world.</p> <div class="row justify-content-sm-center"> <iframe width="800" height="450" src="https://www.youtube.com/embed/RlPQR044pEQ?si=cEpnkozxuI7I2hsS" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> <b>Video:</b> Synthesis and runtime of safety critic filter with ISAACS on quadruped robot. </div> <h1 id="gameplay-filters">Gameplay Filters</h1> <div class="row justify-content-sm-center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/corl-gameplay-frontFigure3-480.webp 480w,/assets/img/corl-gameplay-frontFigure3-800.webp 800w,/assets/img/corl-gameplay-frontFigure3-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/corl-gameplay-frontFigure3.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" style=" height: 300px; width: 100%; object-fit: cover; object-position: center; " title="Gameplay Filters front figure" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> <b>Figure:</b> Snapshots of two different quadruped robots under adversarial conditions, including external tugging forces and unmodeled terrains. The gameplay filter preserves safety across experiments, compared to the unfiltered counterpart. </div> <p>The <b>gameplay filter</b> <a class="citation" href="#nguyen2024gameplay">(Nguyen* et al., 2024)</a> is a new class of predictive safety filters, offering a general approach for runtime robot safety based on game-theoretic reinforcement learning and the core principles of safety filters. Our method learns a best-effort safety policy and a worst-case sim-to-real gap in simulation, and then uses their interplay to inform the robot’s real-time decisions on how and when to preempt potential safety violations.</p> <hr> <h2 id="learn-from-adversity">Learn from adversity</h2> <div class="row"> <div class="col-sm-8 mt-3 mt-md-0"> Our approach first pre-trains a safety-centric control policy in simulation, by pitting it against an adversarial environment agent that is simultaneously learning to steer the robot towards catastrophic failures. This escalation produces a robust robot safety policy that is remarkably hard to exploit, but also an estimate of the worst-case sim-to-real gap that the robot might encounter after deployment. The algorithm updates a safety value network (critic) and keeps a leaderboard of the most effective player policies (actors). </div> <div class="col-sm-4 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rss-spirit-training-480.webp 480w,/assets/img/rss-spirit-training-800.webp 800w,/assets/img/rss-spirit-training-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/rss-spirit-training.gif" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RSS Spirit training in simulation" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/isaacs-training-process-480.webp 480w,/assets/img/isaacs-training-process-800.webp 800w,/assets/img/isaacs-training-process-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/isaacs-training-process.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="ISAACS training process" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>Synthesis</b>: We employ a game-theoretic reach-avoid reinforcement learning scheme that iteratively pits the robot's controller against a simulated adversarial environment. The algorithm updates a safety value network (critic) and keeps a <i>leaderboard</i> of the most effective player policies (actors). </div> </div> </div> <hr> <h2 id="never-lose-a-game">Never lose a game</h2> <p>At runtime, the learned player strategies become part of a safety filter, which allows the robot to pursue its task-specific goals or learn a new policy as long as safety is not in jeopardy, but intervenes as needed to prevent future safety violations.</p> <div class="row justify-content-sm-center"> <div class="col-sm-8 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/gameplay-filter-deployment-diagram-480.webp 480w,/assets/img/gameplay-filter-deployment-diagram-800.webp 800w,/assets/img/gameplay-filter-deployment-diagram-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/gameplay-filter-deployment-diagram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Gameplay Filters deployment diagram" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="caption"> <b>Runtime</b>: Our gameplay filter maintains safety by continually playing out imagined safety games between the best learned controller and disturbance. It <b><i>only</i></b> blocks task-driven actions that could lead to losing future games (i.e., violating safety) and replaces them with the learned safety controls. </div> </div> </div> <p>To decide when and how to intervene, the gameplay filter continually imagines (simulates) hypothetical games between the two learned agents after each candidate task action: if taking the proposed action leads to the robot losing the safety game against the learned adversarial environment, the action is rejected and replaced by the learned safety policy.</p> <hr> <h2 id="results">Results</h2> <h3 id="1-tugging-forces-and-irregular-terrain-evaluation">1. Tugging Forces and Irregular Terrain Evaluation</h3> <p>We evaluate the Gameplay Filters on two quadruped robot platforms, Unitree Go2 and Ghost Robotics S40, across two experimental settings:</p> <ul> <li> <strong>Matched ODD (50 N tugging force)</strong>: A disturbance consistent with the training Operational Design Domain (ODD), designed to assess whether the Gameplay Filter can maintain robust safety without excessively hindering task execution.</li> <li> <strong>Unmodeled terrain</strong>: A deployment scenario outside the training distribution, used to evaluate whether the Gameplay Filter can preserve zero-shot safety under unmodeled conditions.</li> </ul> <p><strong>Gameplay filter on unmodeled terrain</strong></p> <div class="row justify-content-sm-center"> <iframe width="800" height="450" src="https://www.youtube.com/embed/cjRP93HAkDQ?si=2F3o4I_H6jh6IYDT" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> <b>Video:</b> Gameplay Filter on bumpy terrain experiment. </div> <p><strong>Gameplay filter under tugging force</strong></p> <div class="row justify-content-sm-center"> <iframe width="800" height="450" src="https://www.youtube.com/embed/hPhStA7SzmQ?si=zlALtt29NeJ2M37l" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> <b>Video:</b> Gameplay Filter against tugging force experiment. </div> <p><strong>Baseline comparison: Safety critic filter and unfiltered task policy</strong></p> <div class="row justify-content-sm-center"> <iframe width="800" height="450" src="https://www.youtube.com/embed/18CS0Wcbjmk?si=3mbaYSRtmI0WD8TV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> <b>Video:</b> We compare the result of gameplay filter against 2 baselines: Safety critic filter, and unfiltered task policy. Across all experiments, the gameplay filter maintains higher safe rate, and only fail when the adversarial bound exceeds the defined ODD. </div> <h3 id="2-implicit-robustness-against-degradation">2. Implicit robustness against degradation</h3> <p>In this demonstration, the rear right abduction motor was broken. The robot’s task policy and safety filter were unaware of this.</p> <div class="row justify-content-sm-center"> <iframe width="800" height="450" src="https://www.youtube.com/embed/P2bFGkKamnQ?si=SFZipKgcnIPDPRlL" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="caption"> <b>Video:</b> A demonstration showing gameplay filter maintaining safety for the robot with broken motor (rear right abduction) while under tugging force. This highlights the effectiveness of the method, with implicit robustness against degradation. </div> <p>Similarly, when the motors of the Unitree Go2 were <b><u>noticeably degraded</u></b>, with incorrect encoder readings and dampened actuation performance during the CoRL 2024 demo, the manufacturer’s built-in controller could no longer stabilize the robot, causing it to fail on its own.</p> <p>Despite this, our Gameplay Filters solution continued to function, <b><u>demonstrating strong robustness to real-world degradation and adversarial conditions</u></b>.</p> <div class="row justify-content-sm-center"> <div class="col-12 mt-3 mt-md-0"> <div class="row justify-content-center gx-3 gy-3"> <div class="col-12 col-md-auto"> <iframe width="400" height="225" src="https://www.youtube.com/embed/WQg4HhBZ3Nc?si=julaGm88f9kfLkl_" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="col-12 col-md-auto"> <iframe width="400" height="225" src="https://www.youtube.com/embed/Nm4ev8VXWrE?si=gQ1zz-3NAGi0KeFV" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> </div> </div> <div class="caption"> <b>Video:</b> (Left) Manufacturer's built-in controller causes robot from falling by itself after motor and sensor degradation. (Right) Gameplay filter allows the robot to still function safely. </div> <h3 id="3-tackling-large-sim-to-real-gap">3. Tackling large sim-to-real gap</h3> <p>In this demonstration, we train and deploy an RL-driven locomotion task policy on the Unitree Go2 robot with large sim-to-real gap. When deployed on the robot, the task policy causes the robot to flip over. Deploying gameplay filter allows the robot to complete the sequence of task actions without falling.</p> <div class="row justify-content-sm-center"> <div class="col-12 mt-3 mt-md-0"> <div class="row justify-content-center gx-3 gy-3"> <div class="col-12 col-md-auto"> <iframe class="d-block mx-auto" width="400" height="225" src="https://www.youtube.com/embed/hHTRdW3too4?si=W_sV0snn3tyHGdXs" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> <div class="col-12 col-md-auto"> <iframe class="d-block mx-auto" width="400" height="225" src="https://www.youtube.com/embed/IxLHCLyvl8k?si=5yYJNomF2O9v7j0R" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen=""></iframe> </div> </div> </div> </div> <div class="caption"> <b>Video:</b> Sim-to-real deployment of an RL-trained locomotion policy on the Unitree Go2. Due to a large sim-to-real gap, the unfiltered policy causes the robot to flip during execution (left). When augmented with the Gameplay Filter, the robot successfully completes the task sequence without falling (right). </div> <hr> <h2 id="takeaway">Takeaway</h2> <p>Gameplay filters allow robots to maintain robust zero-shot safety across deployment conditions with minimal impact on task performance.</p> <ul> <li style="list-style-type: '✅ '">It only overrides unsafe actions that would cause a safety failure for some realization of uncertainty.</li> <li style="list-style-type: '✅ '">Only requires a single trajectory rollout at each control cycle, enabling runtime safety filtering.</li> <li style="list-style-type: '✅ '">To our knowledge, this is the first successful demonstration of a full-order safety filter for legged robots (36-D).</li> </ul> <p><strong>Key contributions:</strong></p> <ul> <li> <b>Scalable</b>: The filter’s neural network makes it suitable for challenging robotic settings like walking on abrupt terrain and under strong forces.</li> <li> <b>General</b>: A gameplay filter can be synthesized automatically for any robotic system. All you need is a (black-box) dynamics model.</li> <li> <b>Robust</b>: The gameplay filter actively learns and explicitly predicts dangerous discrepancies between the modeled and real dynamics.</li> </ul> </article> <h2>References</h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://www.corl.org/" rel="external nofollow noopener" target="_blank">CoRL</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/gameplay-filters.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="gameplay-filters.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="nguyen2024gameplay" class="col-sm-8"> <div class="title">Gameplay Filters: Robust Zero-Shot Safety through Adversarial Imagination</div> <div class="author"> <em>Duy P. Nguyen<sup>*</sup></em>, Kai-Chieh Hsu<sup>*</sup>, Wenhao Yu, and <span class="more-authors" title="click to view 2 more authors" onclick=" var element=$(this); element.attr('title', ''); var more_authors_text=element.text() == '2 more authors' ? 'Jie Tan, Jaime Fernández Fisac' : '2 more authors'; var cursorPosition=0; var textAdder=setInterval(function(){ element.html(more_authors_text.substring(0, cursorPosition + 1)); if (++cursorPosition == more_authors_text.length){ clearInterval(textAdder); } }, '10'); ">2 more authors</span> </div> <div class="periodical"> <em>In 8th Annual Conference on Robot Learning</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://saferobotics.princeton.edu/research/gameplay-filter" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://arxiv.org/pdf/2405.00846" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:eQOLeE2rZwMC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-18-4285F4?logo=googlescholar&amp;labelColor=beige" alt="18 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>Despite the impressive recent advances in learning-based robot control, ensuring robustness to out-of-distribution conditions remains an open challenge. Safety filters can, in principle, keep arbitrary control policies from incurring catastrophic failures by overriding unsafe actions, but existing solutions for complex (e.g., legged) robot dynamics do not span the full motion envelope and instead rely on local, reduced-order models. These filters tend to overly restrict agility and can still fail when perturbed away from nominal conditions. This paper presents the gameplay filter, a new class of predictive safety filter that continually plays out hypothetical matches between its simulation-trained safety strategy and a virtual adversary co-trained to invoke worst-case events and sim-to-real error, and precludes actions that would cause failures down the line. We demonstrate the scalability and robustness of the approach with a first-of-its-kind full-order safety filter for (36-D) quadrupedal dynamics. Physical experiments on two different quadruped platforms demonstrate the superior zero-shot effectiveness of the gameplay filter under large perturbations such as tugging and unmodeled terrain. Experiment videos and open-source software are available online: https://saferobotics.org/research/gameplay-filter</p> </div> </div> </div> </li></ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"><li> <div class="row"> <div class="col col-sm-2 abbr"> <abbr class="badge rounded w-100" style="background-color:#0041a8"> <a href="https://openreview.net/group?id=L4DC.org" rel="external nofollow noopener" target="_blank">L4DC</a> </abbr> <figure> <picture> <img src="/assets/img/publication_preview/ISAACS.gif" class="preview z-depth-1 rounded" width="100%" height="auto" alt="ISAACS.gif" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <div id="pmlr-v211-hsu23a" class="col-sm-8"> <div class="title">ISAACS: Iterative Soft Adversarial Actor-Critic for Safety</div> <div class="author"> Kai-Chieh Hsu<sup>*</sup>, <em>Duy Phuong Nguyen<sup>*</sup></em>, and Jaime Fernàndez Fisac </div> <div class="periodical"> <em>In Proceedings of The 5th Annual Learning for Dynamics and Control Conference</em>, 15–16 jun 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://saferobotics.princeton.edu/research/isaacs" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://proceedings.mlr.press/v211/hsu23a/hsu23a.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> <a href="https://youtu.be/RlPQR044pEQ?si=BO3vguUSvZc81r9i" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Video</a> </div> <div class="badges"> <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=BpR72EcAAAAJ&amp;citation_for_view=BpR72EcAAAAJ:Tyk-4Ss8FVUC" aria-label="Google Scholar link" role="button" rel="external nofollow noopener" target="_blank"> <img src="https://img.shields.io/badge/scholar-43-4285F4?logo=googlescholar&amp;labelColor=beige" alt="43 Google Scholar citations"> </a> </div> <div class="abstract hidden"> <p>The deployment of robots in uncontrolled environments requires them to operate robustly under previously unseen scenarios, like irregular terrain and wind conditions. Unfortunately, while rigorous safety frameworks from robust optimal control theory scale poorly to high-dimensional nonlinear dynamics, control policies computed by more tractable "deep" methods lack guarantees and tend to exhibit little robustness to uncertain operating conditions. This work introduces a novel approach enabling scalable synthesis of robust safety-preserving controllers for robotic systems with general nonlinear dynamics subject to bounded modeling error by combining game-theoretic safety analysis with adversarial reinforcement learning in simulation. Following a soft actor-critic scheme, a safety-seeking fallback policy is co-trained with an adversarial "disturbance" agent that aims to invoke the worst-case realization of model error and training-to-deployment discrepancy allowed by the designer’s uncertainty. While the learned control policy does not intrinsically guarantee safety, it is used to construct a real-time safety filter (or shield) with robust safety guarantees based on forward reachability rollouts. This shield can be used in conjunction with a safety-agnostic control policy, precluding any task-driven actions that could result in loss of safety. We evaluate our learning-based safety approach in a 5D race car simulator, compare the learned safety policy to the numerically obtained optimal solution, and empirically validate the robust safety guarantee of our proposed safety shield against worst-case model discrepancy.</p> </div> </div> </div> </li></ol> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2026 Duy P. Nguyen. Last updated: January 17, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>